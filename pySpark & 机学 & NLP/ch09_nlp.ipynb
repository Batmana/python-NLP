{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patent-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch09 自然语言处理\n",
    "# 切词\n",
    "df = spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                         (2,'I would recommend this movie to my friends'),\n",
    "                         (3,'movie was alright but acting was horrible'),\n",
    "                         (4,'I am never watching that movie ever again')],\n",
    "                        ['user_id', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "anticipated-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|user_id|review                                    |tokens                                             |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|1      |I really liked this movie                 |[i, really, liked, this, movie]                    |\n",
      "|2      |I would recommend this movie to my friends|[i, would, recommend, this, movie, to, my, friends]|\n",
      "|3      |movie was alright but acting was horrible |[movie, was, alright, but, acting, was, horrible]  |\n",
      "|4      |I am never watching that movie ever again |[i, am, never, watching, that, movie, ever, again] |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark引入Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization = Tokenizer(inputCol = 'review', outputCol = 'tokens')\n",
    "tokenized_df = tokenization.transform(df)\n",
    "tokenized_df.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "shaped-contribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 移除停用词\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', outputCol='refined_tokens')\n",
    "refined_df = stopword_removal.transform(tokenized_df)\n",
    "refined_df.select(['user_id', 'tokens', 'refined_tokens']).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "figured-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+--------------------------------+\n",
      "|user_id|refined_tokens                    |features                        |\n",
      "+-------+----------------------------------+--------------------------------+\n",
      "|1      |[really, liked, movie]            |(11,[0,4,7],[1.0,1.0,1.0])      |\n",
      "|2      |[recommend, movie, friends]       |(11,[0,1,10],[1.0,1.0,1.0])     |\n",
      "|3      |[movie, alright, acting, horrible]|(11,[0,5,6,9],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(11,[0,2,3,8],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 词袋，计数向量器\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vec = CountVectorizer(inputCol='refined_tokens', outputCol = 'features')\n",
    "cv_df = count_vec.fit(refined_df).transform(refined_df)\n",
    "cv_df.select(['user_id', 'refined_tokens', 'features']).show(4, False)\n",
    "# 向量长度11位（词袋大小，one-hot），第一行数据有三个值，位置是0，4，7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "perfect-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'horrible',\n",
       " 'liked',\n",
       " 'really',\n",
       " 'watching',\n",
       " 'alright',\n",
       " 'friends',\n",
       " 'recommend',\n",
       " 'ever',\n",
       " 'never',\n",
       " 'acting']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 展示词典\n",
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beautiful-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|user_id|refined_tokens                    |tf_features                                            |\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "|1      |[really, liked, movie]            |(262144,[14,32675,155321],[1.0,1.0,1.0])               |\n",
      "|2      |[recommend, movie, friends]       |(262144,[129613,155321,222394],[1.0,1.0,1.0])          |\n",
      "|3      |[movie, alright, acting, horrible]|(262144,[80824,155321,236263,240286],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(262144,[63139,155321,203802,245806],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashing_vec = HashingTF(inputCol='refined_tokens', outputCol='tf_features')\n",
    "hashing_df = hashing_vec.transform(refined_df)\n",
    "hashing_df.select(['user_id', 'refined_tokens', 'tf_features']).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strategic-registration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|user_id|tf_idf_features                                                                                     |\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "|1      |(262144,[14,32675,155321],[0.9162907318741551,0.9162907318741551,0.0])                              |\n",
      "|2      |(262144,[129613,155321,222394],[0.9162907318741551,0.0,0.9162907318741551])                         |\n",
      "|3      |(262144,[80824,155321,236263,240286],[0.9162907318741551,0.0,0.9162907318741551,0.9162907318741551])|\n",
      "|4      |(262144,[63139,155321,203802,245806],[0.9162907318741551,0.0,0.9162907318741551,0.9162907318741551])|\n",
      "+-------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算IDF\n",
    "tf_idf_vec = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "tf_idf = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf.select('user_id', 'tf_idf_features').show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nutritional-correlation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用机器学习进行文本分类\n",
    "text_df = spark.read.csv('Movie_reviews.csv', inferSchema=True, header=True, sep=',')\n",
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invalid-senate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accomplished-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取被正确标记的记录\n",
    "text_df = text_df.filter((text_df.Sentiment=='1')| (text_df.Sentiment=='0'))\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "agreed-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "present-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+\n",
      "|rand(3948031753215614132)|count|\n",
      "+-------------------------+-----+\n",
      "|0.177254188557608        |1    |\n",
      "|0.50533733000834         |1    |\n",
      "|0.48382393572454807      |1    |\n",
      "|0.18270550609738534      |1    |\n",
      "|0.5983687619538715       |1    |\n",
      "|0.5535985762803357       |1    |\n",
      "|0.5376104088943879       |1    |\n",
      "|0.47355665865667795      |1    |\n",
      "|0.11478891688308612      |1    |\n",
      "|0.9395946717402214       |1    |\n",
      "+-------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "text_df.groupby(rand()).count().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reduced-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+\n",
      "|Review                                                                  |Label|\n",
      "+------------------------------------------------------------------------+-----+\n",
      "|Is it just me, or does Harry Potter suck?...                            |0.0  |\n",
      "|and i hate Harry Potter.                                                |0.0  |\n",
      "|Because I would like to make friends who like the same things I like, an|1.0  |\n",
      "|Combining the opinion / review from Gary and Gin Zen, The Da Vinci Code |0.0  |\n",
      "|Which is why i said silent hill turned into reality coz i was hella like|1.0  |\n",
      "|I, too, like Harry Potter..                                             |1.0  |\n",
      "|the last stand and Mission Impossible 3 both were awesome movies.       |1.0  |\n",
      "|Brokeback Mountain was boring.                                          |0.0  |\n",
      "|meganpenworthy dressed as a character from Harry Potter and the Selfish |0.0  |\n",
      "|Me, I like the Harry Potter movies but the books have no real appeal for|1.0  |\n",
      "+------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast(\"float\")).drop('Sentiment')\n",
    "text_df.orderBy(rand()).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "governmental-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Review                                                                  |Label|length|\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Because I would like to make friends who like the same things I like, an|1.0  |72    |\n",
      "|man i loved brokeback mountain!                                         |1.0  |31    |\n",
      "|Then snuck into Brokeback Mountain, which is the most depressing movie I|0.0  |72    |\n",
      "|I like Mission Impossible movies because you never know who's on the rig|1.0  |72    |\n",
      "|I either LOVE Brokeback Mountain or think it's great that homosexuality |1.0  |71    |\n",
      "|\"I liked the first \"\" Mission Impossible.\"                              |1.0  |42    |\n",
      "|I hate Harry Potter, it's retarted, gay and stupid and there's only one |0.0  |71    |\n",
      "|Brokeback Mountain is a beautiful movie..                               |1.0  |41    |\n",
      "|The Da Vinci Code was absolutely AWESOME!                               |1.0  |41    |\n",
      "|I wanted desperately to love'The Da Vinci Code as a film.               |1.0  |57    |\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 增加一个额外的length列\n",
    "from pyspark.sql.functions import length\n",
    "text_df=text_df.withColumn('length', length(text_df['Review']))\n",
    "text_df.orderBy(rand()).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fatal-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Label').agg({'Length': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "declared-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization = Tokenizer(inputCol='Review', outputCol='tokens')\n",
    "tokenized_df = tokenization.transform(text_df)\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', outputCol='refined_tokens')\n",
    "refined_text_df = stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sticky-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捕获评论中的标记数量（分词，去停用词后的数量）\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "isolated-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf=udf(lambda s:len(s), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thirty-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|              Review|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|man i loved broke...|  1.0|    31|[man, i, loved, b...|[man, loved, brok...|          4|\n",
      "|mission impossibl...|  1.0|    37|[mission, impossi...|[mission, impossi...|          4|\n",
      "|The Da Vinci Code...|  1.0|    57|[the, da, vinci, ...|[da, vinci, code,...|          7|\n",
      "|Brokeback Mountai...|  0.0|    30|[brokeback, mount...|[brokeback, mount...|          3|\n",
      "|I LOVE Harry Pott...|  1.0|    22|[i, love, harry, ...|[love, harry, pot...|          3|\n",
      "|da vinci code was...|  1.0|    37|[da, vinci, code,...|[da, vinci, code,...|          5|\n",
      "|Which is why i sa...|  1.0|    72|[which, is, why, ...|[said, silent, hi...|          8|\n",
      "|The Da Vinci Code...|  0.0|    34|[the, da, vinci, ...|[da, vinci, code,...|          6|\n",
      "|i loved brokeback...|  1.0|    29|[i, loved, brokeb...|[loved, brokeback...|          3|\n",
      "|Da Vinci Code suc...|  0.0|    25|[da, vinci, code,...|[da, vinci, code,...|          5|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col函数：根据给定的列名返回一个列。\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n",
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "brutal-bouquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------+-----+\n",
      "|token_count|features                                                                          |Label|\n",
      "+-----------+----------------------------------------------------------------------------------+-----+\n",
      "|5          |(2302,[0,1,4,43,236],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|9          |(2302,[11,51,229,237,275,742,824,1087,1250],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|1.0  |\n",
      "|5          |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|5          |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |1.0  |\n",
      "|8          |(2302,[0,1,4,53,655,1339,1427,1449],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |1.0  |\n",
      "|6          |(2302,[46,229,271,1150,1990,2203],[1.0,1.0,1.0,1.0,1.0,1.0])                      |1.0  |\n",
      "|8          |(2302,[0,1,22,30,111,219,389,535],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |1.0  |\n",
      "|7          |(2302,[0,1,4,228,1258,1716,2263],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |1.0  |\n",
      "|6          |(2302,[0,1,4,33,226,258],[1.0,1.0,1.0,1.0,1.0,1.0])                               |1.0  |\n",
      "|7          |(2302,[0,1,4,223,226,228,262],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |1.0  |\n",
      "+-----------+----------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(inputCol='refined_tokens', outputCol='features')\n",
    "cv_text_df = count_vec.fit(refined_text_df).transform(refined_text_df)\n",
    "#cv_text_df.head()\n",
    "cv_text_df.select(['token_count', 'features', 'Label']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "every-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- token_count: integer (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_df = cv_text_df.select(['features', 'token_count', 'Label'])\n",
    "# 使用VectorAssembler 创建特征\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['features', 'token_count'], outputCol = 'features_vec')\n",
    "model_text_df=df_assembler.transform(model_text_df)\n",
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "collected-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2910|\n",
      "|  0.0| 2358|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用LR分类器\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# 区分训练集\n",
    "train_df, test_df = model_text_df.randomSplit([0.75, 0.25])\n",
    "train_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "severe-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  999|\n",
      "|  0.0|  723|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "oriental-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features_vec', labelCol='Label').fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "martial-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(2302,[0,1,4,5,89...|          9|  1.0|(2303,[0,1,4,5,89...|[-16.546253522916...|[6.51708477704275...|       1.0|\n",
      "|(2302,[0,1,4,5,30...|          5|  1.0|(2303,[0,1,4,5,30...|[-24.003411758523...|[3.76227664317585...|       1.0|\n",
      "|(2302,[0,1,4,5,44...|          5|  1.0|(2303,[0,1,4,5,44...|[-22.822384754074...|[1.22564370045345...|       1.0|\n",
      "|(2302,[0,1,4,5,65...|          5|  1.0|(2303,[0,1,4,5,65...|[-15.815520954707...|[1.35333857882513...|       1.0|\n",
      "|(2302,[0,1,4,5,82...|          6|  1.0|(2303,[0,1,4,5,82...|[-15.915751409526...|[1.22426922994062...|       1.0|\n",
      "|(2302,[0,1,4,10,1...|         10|  0.0|(2303,[0,1,4,10,1...|[34.1923988502912...|[0.99999999999999...|       0.0|\n",
      "|(2302,[0,1,4,11,2...|         10|  0.0|(2303,[0,1,4,11,2...|[22.0316789589850...|[0.99999999972975...|       0.0|\n",
      "|(2302,[0,1,4,12,1...|          8|  1.0|(2303,[0,1,4,12,1...|[-19.917011458998...|[2.23950391698507...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "|(2302,[0,1,4,12,3...|          5|  1.0|(2303,[0,1,4,12,3...|[-21.546015536652...|[4.39222263015524...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=log_reg.evaluate(test_df).predictions\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "indian-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# tp 预测真，真实也真\n",
    "true_postives = results[(results.Label == 1) & (results.prediction ==1)].count()\n",
    "# fp 预测真，真实假\n",
    "false_postives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "# tn\n",
    "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
    "# fn\n",
    "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cultural-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98998998998999\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives) / (float(true_postives) + float(false_negatives))\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "miniature-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696078431372549\n"
     ]
    }
   ],
   "source": [
    "# 精确度\n",
    "precision = float(true_postives) / (true_postives + false_postives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "accessory-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761904761904762\n"
     ]
    }
   ],
   "source": [
    "# 准确率\n",
    "acccuracy = float((true_postives + true_negatives)/ results.count())\n",
    "print(acccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-priest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
